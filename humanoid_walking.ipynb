{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Humanoid Walking Simulation with AnyWidget\n",
    "\n",
    "This notebook demonstrates a humanoid walking simulation running in a widget. We can interact with it from Python to collect data and train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "from humanoid_walker.humanoid_walker import HumanoidWalker\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the widget\n",
    "walker = HumanoidWalker()\n",
    "walker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generating Data\n",
    "\n",
    "We can send actions to the walker and record the state. Let's try sending random actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_records = []\n",
    "\n",
    "walker.reset_simulation()\n",
    "time.sleep(1) # Wait for reset\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(200):\n",
    "    # Random action\n",
    "    action = {\n",
    "        'left_hip': np.random.uniform(-50, 50),\n",
    "        'left_knee': np.random.uniform(-50, 50),\n",
    "        'right_hip': np.random.uniform(-50, 50),\n",
    "        'right_knee': np.random.uniform(-50, 50)\n",
    "    }\n",
    "    walker.apply_action(action)\n",
    "    \n",
    "    # Capture state\n",
    "    state = walker.state\n",
    "    if state:\n",
    "        record = state.copy()\n",
    "        record.update(action)\n",
    "        record['timestamp'] = time.time() - start_time\n",
    "        data_records.append(record)\n",
    "    \n",
    "    time.sleep(0.05) # 20Hz control loop\n",
    "\n",
    "df = pd.DataFrame(data_records)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Learning to Walk\n",
    "\n",
    "We will use a simple Evolutionary Strategy (ES) to learn a policy that maximizes the distance traveled to the right.\n",
    "Policy: Linear mapping from State -> Action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(weights, state_vec):\n",
    "    # state_vec: [torso_angle, left_thigh_angle, left_calf_angle, ...]\n",
    "    # simple linear policy\n",
    "    action_vec = np.tanh(np.dot(weights, state_vec)) * 100 # scale to torque\n",
    "    return {\n",
    "        'left_hip': action_vec[0],\n",
    "        'left_knee': action_vec[1],\n",
    "        'right_hip': action_vec[2],\n",
    "        'right_knee': action_vec[3]\n",
    "    }\n",
    "\n",
    "def run_episode(weights, steps=100):\n",
    "    walker.reset_simulation()\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    initial_x = walker.state.get('torso_x', 200)\n",
    "    \n",
    "    for _ in range(steps):\n",
    "        s = walker.state\n",
    "        if not s:\n",
    "            time.sleep(0.05)\n",
    "            continue\n",
    "            \n",
    "        # Construct state vector (relative angles are better, but using raw for simplicity)\n",
    "        # Normalize roughly\n",
    "        state_vec = np.array([\n",
    "            s.get('torso_angle', 0),\n",
    "            s.get('left_thigh_angle', 0),\n",
    "            s.get('left_calf_angle', 0),\n",
    "            s.get('right_thigh_angle', 0),\n",
    "            s.get('right_calf_angle', 0),\n",
    "            1.0 # bias\n",
    "        ])\n",
    "        \n",
    "        action = get_action(weights, state_vec)\n",
    "        walker.apply_action(action)\n",
    "        time.sleep(0.05)\n",
    "        \n",
    "    final_x = walker.state.get('torso_x', initial_x)\n",
    "    return final_x - initial_x\n",
    "\n",
    "# Initialize weights (4 actions x 6 state vars)\n",
    "best_weights = np.random.randn(4, 6) * 0.1\n",
    "best_reward = -float('inf')\n",
    "\n",
    "# Simple Random Search / Mutation loop\n",
    "for i in range(10):\n",
    "    # Mutate\n",
    "    candidate_weights = best_weights + np.random.randn(4, 6) * 0.5\n",
    "    reward = run_episode(candidate_weights, steps=50)\n",
    "    \n",
    "    print(f\"Episode {i}: Reward {reward:.2f}\")\n",
    "    \n",
    "    if reward > best_reward:\n",
    "        best_reward = reward\n",
    "        best_weights = candidate_weights\n",
    "        print(\"New best!\")\n",
    "\n",
    "print(\"Training done. Running best policy...\")\n",
    "run_episode(best_weights, steps=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
